import { createClient } from '@supabase/supabase-js'
import * as fs from 'fs'
import * as path from 'path'
import * as dotenv from 'dotenv'
import { parse } from 'csv-parse/sync'

dotenv.config({ path: '.env.local' })

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!
const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY!

if (!supabaseUrl || !supabaseKey) {
  console.error('âŒ Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
  process.exit(1)
}

const supabase = createClient(supabaseUrl, supabaseKey)

// ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
const BATCH_SIZE = 10  // ë™ì‹œì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜

interface CsvRow {
  id: string
  title: string
  description: string
  desc_un_html: string
  folder_name: string
  category_name: string
  slug: string
  folder_slug: string
  category_slug: string
  path: string
  status: string
}

// ì „ì—­ ìºì‹œ (ëª¨ë“  ì œí’ˆì—ì„œ ê³µìœ )
const globalCategoryCache = new Map<string, string>()
const globalFolderCache = new Map<string, string>()

async function ensureCategory(product: string, categoryName: string, categorySlug: string) {
  const cacheKey = `${product}:${categorySlug}`

  if (globalCategoryCache.has(cacheKey)) {
    return globalCategoryCache.get(cacheKey)!
  }

  const { data: existing } = await supabase
    .from('categories')
    .select('id')
    .eq('slug', categorySlug)
    .eq('product', product)
    .maybeSingle()

  if (existing) {
    globalCategoryCache.set(cacheKey, existing.id)
    return existing.id
  }

  const { data, error } = await supabase
    .from('categories')
    .insert({
      product,
      slug: categorySlug,
      name_ko: categoryName,
      name_en: categoryName,
      published: true
    })
    .select('id')
    .single()

  if (error) {
    throw error
  }

  globalCategoryCache.set(cacheKey, data.id)
  return data.id
}

async function ensureFolder(
  product: string,
  categoryId: string,
  folderName: string,
  folderSlug: string
) {
  const cacheKey = `${product}:${categoryId}:${folderSlug}`

  if (globalFolderCache.has(cacheKey)) {
    return globalFolderCache.get(cacheKey)!
  }

  const { data: existing } = await supabase
    .from('folders')
    .select('id')
    .eq('slug', folderSlug)
    .eq('category_id', categoryId)
    .eq('product', product)
    .maybeSingle()

  if (existing) {
    globalFolderCache.set(cacheKey, existing.id)
    return existing.id
  }

  const { data, error } = await supabase
    .from('folders')
    .insert({
      product,
      category_id: categoryId,
      slug: folderSlug,
      name_ko: folderName,
      name_en: folderName,
      published: true
    })
    .select('id')
    .single()

  if (error) {
    throw error
  }

  globalFolderCache.set(cacheKey, data.id)
  return data.id
}

async function importDocument(
  row: CsvRow,
  product: string,
  categoryId: string,
  folderId: string
) {
  const csvId = parseInt(row.id)

  // ê¸°ì¡´ ë¬¸ì„œ í™•ì¸
  const { data: existing } = await supabase
    .from('documents')
    .select('id')
    .eq('csv_id', csvId)
    .eq('product', product)
    .maybeSingle()

  if (existing) {
    return { status: 'skip', csvId }
  }

  // ìƒˆ ë¬¸ì„œ ìƒì„± (ì˜ë¬¸ë§Œ, ë²ˆì—­ì€ ë‚˜ì¤‘ì—)
  const { error } = await supabase
    .from('documents')
    .insert({
      csv_id: csvId,
      product,
      category_id: categoryId,
      folder_id: folderId,
      slug: row.slug,
      short_slug: row.slug,
      full_path: row.path,

      // ì˜ë¬¸ (ì›ë³¸)
      title_en: row.title,
      content_html_en: row.description,
      content_text_en: row.desc_un_html,

      // í•œê¸€ (ë²ˆì—­ ì „ì´ë¯€ë¡œ NULL)
      title_ko: null,
      content_html_ko: null,
      content_text_ko: null,

      // ê²€ìƒ‰ ë²¡í„° (ì„ë² ë”© ì „ì´ë¯€ë¡œ NULL)
      search_vector_ko: null,

      published: row.status === '2' // status 2 = published
    })

  if (error) {
    return { status: 'error', csvId, error: error.message }
  }

  return { status: 'success', csvId }
}

async function processDocument(row: CsvRow, product: string) {
  try {
    // ì¹´í…Œê³ ë¦¬ í™•ë³´
    const categoryId = await ensureCategory(product, row.category_name, row.category_slug)

    // í´ë” í™•ë³´
    const folderId = await ensureFolder(product, categoryId, row.folder_name, row.folder_slug)

    // ë¬¸ì„œ ì„í¬íŠ¸
    return await importDocument(row, product, categoryId, folderId)
  } catch (error: any) {
    return { status: 'error', csvId: parseInt(row.id), error: error.message }
  }
}

async function importCSVParallel(csvPath: string, product: string) {
  console.log(`\nğŸ“‚ ${product.toUpperCase()}: ${path.basename(csvPath)}`)

  const content = fs.readFileSync(csvPath, 'utf-8')
  const rows: CsvRow[] = parse(content, {
    columns: true,
    skip_empty_lines: true,
    trim: true,
    relax_quotes: true
  })

  console.log(`  ğŸ“Š ${rows.length.toLocaleString()}ê°œ ë¬¸ì„œ ë°œê²¬`)

  let successCount = 0
  let skipCount = 0
  let errorCount = 0

  // ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬
  for (let i = 0; i < rows.length; i += BATCH_SIZE) {
    const batch = rows.slice(i, i + BATCH_SIZE)

    // ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬
    const results = await Promise.all(
      batch.map(row => processDocument(row, product))
    )

    // ê²°ê³¼ ì§‘ê³„
    results.forEach(result => {
      if (result.status === 'success') successCount++
      else if (result.status === 'skip') skipCount++
      else errorCount++
    })

    // ì§„í–‰ ìƒí™© ì¶œë ¥
    const processed = Math.min(i + BATCH_SIZE, rows.length)
    process.stdout.write(
      `\r  [${processed}/${rows.length}] ` +
      `âœ… ${successCount} â­ï¸  ${skipCount} âŒ ${errorCount}`
    )
  }

  console.log(`\n  âœ… ì™„ë£Œ: ì„±ê³µ ${successCount.toLocaleString()}, ìŠ¤í‚µ ${skipCount.toLocaleString()}, ì‹¤íŒ¨ ${errorCount.toLocaleString()}`)

  return { successCount, skipCount, errorCount }
}

async function main() {
  console.log('ğŸš€ ì „ì²´ ì œí’ˆ ë³‘ë ¬ ì„í¬íŠ¸ ì‹œì‘\n')
  console.log('=' .repeat(60))
  console.log('ğŸ“‹ ì „ëµ: ì˜ë¬¸ ì›ë³¸ë§Œ ì €ì¥ â†’ ë²ˆì—­ì€ ë‚˜ì¤‘ì—')
  console.log(`âš¡ ë³‘ë ¬ ì²˜ë¦¬: ${BATCH_SIZE}ê°œì”© ë™ì‹œ ì²˜ë¦¬`)
  console.log('ğŸ“¦ ëŒ€ìƒ: Freshdesk, Freshservice, Freshsales, Freshchat, Freshcaller')
  console.log('=' .repeat(60))

  const startTime = Date.now()
  let totalSuccess = 0
  let totalSkip = 0
  let totalError = 0

  const products = [
    {
      name: 'freshdesk',
      files: ['data/freshdesk/merged_articles_links_replaced_freshdesk.csv']
    },
    {
      name: 'freshservice',
      files: [
        'data/freshservice/merged_articles_links_replaced_freshservice_part1.csv',
        'data/freshservice/merged_articles_links_replaced_freshservice_part2.csv',
        'data/freshservice/merged_articles_links_replaced_freshservice_part3.csv',
        'data/freshservice/merged_articles_links_replaced_freshservice_part4.csv',
        'data/freshservice/merged_articles_links_replaced_freshservice_part5.csv'
      ]
    },
    {
      name: 'freshsales',
      files: ['data/Freshsales/merged_articles_links_replaced_freshsales.csv']
    },
    {
      name: 'freshchat',
      files: ['data/Freshchat/merged_articles_links_replaced_freshchat.csv']
    },
    {
      name: 'freshcaller',
      files: ['data/Freshcaller/merged_articles_links_replaced_freshcaller.csv']
    }
  ]

  for (const product of products) {
    console.log(`\n${'='.repeat(60)}`)
    console.log(`ğŸ“¦ ${product.name.toUpperCase()} ì²˜ë¦¬ (${product.files.length}ê°œ íŒŒì¼)`)
    console.log('='.repeat(60))

    for (const file of product.files) {
      const filePath = path.join(process.cwd(), file)

      if (!fs.existsSync(filePath)) {
        console.log(`âš ï¸  íŒŒì¼ ì—†ìŒ: ${file}`)
        continue
      }

      const result = await importCSVParallel(filePath, product.name)
      totalSuccess += result.successCount
      totalSkip += result.skipCount
      totalError += result.errorCount
    }
  }

  const endTime = Date.now()
  const totalSeconds = Math.floor((endTime - startTime) / 1000)
  const totalMinutes = Math.floor(totalSeconds / 60)
  const remainingSeconds = totalSeconds % 60

  console.log('\n\n' + '='.repeat(60))
  console.log('ğŸ“Š ì „ì²´ ì„í¬íŠ¸ ì™„ë£Œ í†µê³„')
  console.log('='.repeat(60))
  console.log(`âœ… ì„±ê³µ: ${totalSuccess.toLocaleString()}ê±´`)
  console.log(`â­ï¸  ìŠ¤í‚µ: ${totalSkip.toLocaleString()}ê±´ (ì´ë¯¸ ì¡´ì¬)`)
  console.log(`âŒ ì‹¤íŒ¨: ${totalError.toLocaleString()}ê±´`)
  console.log(`ğŸ“Š ì´ê³„: ${(totalSuccess + totalSkip + totalError).toLocaleString()}ê±´`)
  console.log(`â±ï¸  ì†Œìš” ì‹œê°„: ${totalMinutes}ë¶„ ${remainingSeconds}ì´ˆ`)
  console.log(`âš¡ ë³‘ë ¬ íš¨ìœ¨: ${BATCH_SIZE}x ë°°ì¹˜ ì²˜ë¦¬`)
  console.log('='.repeat(60))

  console.log('\nâœ… ì „ì²´ ì˜ë¬¸ ì›ë³¸ ë°ì´í„° êµ¬ì¶• ì™„ë£Œ!')
  console.log('\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:')
  console.log('  1. npx tsx scripts/data/check-db-status.ts')
  console.log('     â†’ ì œí’ˆë³„/ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ í™•ì¸')
  console.log('  2. ì¤‘ìš” ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ ê²°ì •')
  console.log('  3. ìš°ì„ ìˆœìœ„ë³„ ì²´ê³„ì  ë²ˆì—­ ì‹œì‘')
}

main().catch(console.error)
